{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# MARKOV CHAINS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WHAT ARE MARKOV CHAINS??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A stochastic process containing random variables, transitioning from one state to another depending on certain assumptions and definite probabilistic rules.\n",
    "These random variables transition from one to state to the other, based on an important mathematical property called Markov Property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discrete Time Markov Property states that the calculated probability of a random process transitioning to the next possible state is only dependent on the current state and time and it is independent of the series of states that preceded it.\n",
    "\n",
    "The fact that the next possible action/ state of a random process does not depend on the sequence of prior states, renders Markov chains as a memory-less process that solely depends on the current state/action of a variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this means is, we will have an “agent” that randomly jumps around different states, with a certain probability of going from each state to another one.\n",
    "\n",
    "To show what a Markov Chain looks like, we can use a digraph, where each node is a state (with a label or associated data), and the weight of the edge that goes from node a to node b is the probability of jumping from state a to state b.\n",
    "Here’s an example,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically:\n",
    "Let the random process be, {Xm, m=0,1,2,⋯}.\n",
    "\n",
    "This process is a Markov chain only if,"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"ma1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for all m, j, i, i0, i1, ⋯ im−1\n",
    "\n",
    "For a finite number of states, S={0, 1, 2, ⋯, r}, this is called a finite Markov chain.\n",
    "\n",
    "The left hand side of this equation is giving the probability of being in State  i  at period  t+1 , given that we were in State  j  at period  t . In our case above, the probability of being at State 1 at time  t+1  given that we are in State 2 at time  t  is .1. Then, the right hand side of the equation is the probability of being at State  i  at time  t+1  given the entire past history of the chain, or the location of the particle in every single State (here, the State  j  at time  t , and all the way down to  x0 , which is just arbitrary notation for some constant that  X0  takes on at time 0).\n",
    "\n",
    "The Markov Property states that these sides are equal; that is, knowing where you were in the previous period makes the rest of the chain history irrelevant. If we know  Xt=j , then the rest of the history doesn’t add any information for predicting  Xt+1 ; that’s why the two sides are equal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov Chains are actually extremely intuitive. Formally, they are examples of Stochastic Processes, or random variables that evolve over time. Here is a basic but classic example of what a Markov chain can actually look like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # State Diagram "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"state_diag1.jpeg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this simple example, we’re assuming that we can land in either be 1,2 or 3 . If the current state is 1, then  there is a .2 probability that we will transit to state 2, and a .5 probability that we remains in the same state .If in state 2, then  there is a .5 probability that it reamins in the state again  and a .4 probability that we land in 3 next move. We can envision a particle bouncing around this chain, moving according to the probabilities drawn here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transition Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transition Matrix, which we usually notate as  Q .This is simply the matrix that contains the probabilities of, you guessed it, transferring between nodes. In this case, the transition matrix looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"transition_matrix.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are also intuitive to read: the  {ith,jth}  entry (the value in the  ith  row and  jth  column) marks the probability of going from State  i  to State  j . In this case, we named the states instead of numbering them.Here, then, we associate the first row and first column with the '1' state, and the second row and column with the '2' state and the third row and column with the '3' state .\n",
    "\n",
    "For example, the probability of going from '1' to '2' is coded here in the first row and second column  is 0.1  ,  the probability of going from '2' to '3' is coded here in the second row and third column  is 0.2 , probability of remaining in the state '2'  is 0.5. In this way, we can find the probability of going from a state to another state from Transition Matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s important to note, again, that the rows of these transition matrices must sum to 1. That’s because from a certain state, we must go somewhere. Again, if these probabilities added up to something like .6, we wouldn’t know where the state traveled to 40 %  of the time! These transition matrices will also always be square (i.e., same number of rows as columns) since we want to keep track of the probability of going from every state to every other state, and they will always have the same number of rows (and same number of columns) as the number of states in the chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variety of descriptions of either a specific state in a Markov chain or the entire Markov chain allow for better understanding of the Markov chain's behavior. Let P be the transition matrix of Markov chain {X0,X1,...}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A state i has period  k≥1 if any chain starting at and        returning to state i with positive probability must take a number of steps divisible by k. If k=1, then the state is known as aperiodic, and if k > 1, the state is known as periodic. If all states are aperiodic, then the Markov chain is known as aperiodic.\n",
    "\n",
    "* A Markov chain is known as irreducible if there exists a chain of steps between any two states that has positive probability.\n",
    "\n",
    "* An absorbing state i is a state for which P_{i,i} = 1 An absorbing Markov chain is a Markov chain in which it is impossible to leave some states, and any state could (after some number of steps, with positive probability) reach such a state.\n",
    "\n",
    "* A state is known as recurrent or transient depending upon whether or not the Markov chain will eventually return to it. A recurrent state is known as positive recurrent if it is expected to return within a finite number of steps, and null recurrent otherwise.  There is some possibility (a nonzero probability) that a process beginning in a transient state will never return to that state. There is a guarantee that a process beginning in a recurrent state will return to that state. Transience and recurrence issues are central to the study of Markov chains and help describe the Markov chain's overall structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Here, taken a text file having speech of US president, Biden. This model helps to predict next few lines by taking single word as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data set\n",
    "corpus=open('C:/Users/pksds/Documents/mycourse/practice_python/biden_speech.txt',encoding='utf8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data set into individual words\n",
    "corpus = corpus.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Next, creating a function that generates the different pairs of words in the speeches. To save up space, \n",
    "we’ll use a generator object.Here, we are creating a pair of every adjacent words to form a tuple which will \n",
    "be used to make prediction in later stages .\n",
    "'''\n",
    "def make_pairs(corpus):\n",
    "    for i in range(len(corpus) - 1):\n",
    "        yield (corpus[i], corpus[i + 1])\n",
    "pairs = make_pairs(corpus)\n",
    "\n",
    "#As shown below are few pairs of adjacent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here,  initializing an empty dictionary to store the pairs of words.\n",
    "\n",
    "In case the first word in the pair is already a key in the dictionary, just append the next potential \n",
    "word to the list of words that follow the word. But if the word is not a key, then create a new entry in the dictionary \n",
    "and assign the key equal to the first word in the pair.\n",
    "\n",
    "'''\n",
    "word_dict = {}\n",
    "for word_1, word_2 in pairs:\n",
    "    if word_1 in word_dict.keys():\n",
    "        word_dict[word_1].append(word_2)\n",
    "    else:\n",
    "        word_dict[word_1] = [word_2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['America']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Random choice will give any word which doesnt lead to good prediction so instead of taking any word \n",
    ",creating  a function which will give only starting word of new sentence (assuming starting word of new sentence starts\n",
    "with capital letter in the transcipt) .\n",
    "'''\n",
    "\n",
    "\n",
    "first_word = np.random.choice(corpus) #Initally ,randomly pick the first word\n",
    "chain = [first_word]\n",
    " \n",
    "#Pick the first word as a capitalized word so that the picked word is not taken from in between a sentence\n",
    "def capitalized_word(first_word):\n",
    "    while first_word.islower():\n",
    "        first_word = np.random.choice(corpus)\n",
    " \n",
    "#Start the chain from the picked word\n",
    "        chain = [first_word]\n",
    "    return chain\n",
    "    \n",
    "\n",
    "\n",
    "chain=capitalized_word(first_word) #here, it seen that capital word is optained\n",
    "\n",
    "\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Following the first word, each word in the chain is randomly sampled from the list of words which have followed \n",
    "that specific word in biden’s live speeches and its appended to list chain\n",
    "\n",
    "'''\n",
    "n_words = 30  #Initialize the number of stimulated words\n",
    "for i in range(n_words):\n",
    "    chain.append(np.random.choice(word_dict[chain[-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "America have come to celebrate peace, to do whatever we never, ever, ever said those words. And all know about bringing the years — there’s still more work to be back.\n"
     ]
    }
   ],
   "source": [
    "#The simulated words are displayed\n",
    "print(' '.join(chain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " So,We can conclude from the above activity that random process transitioning to the next possible state (getting next \n",
    " word) is only dependent on the current state  and it is independent of the series of states that preceded it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More on Markov Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ERGODIC MARKOV CHAINS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Markov chains converge in the long run to a single stationary distribution? No. It turns out only a special type of Markov chains called ergodic Markov chains will converge like this to a single distribution. An ergodic Markov chain is a Markov chain that satisfies two special conditions: it’s both irreducible and aperiodic. I’ll explain what these mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Condition 1: Irreducible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to be able to get be able to get from any one state to any other state eventually. We can never get permanently stuck in one state or a set of states. When this is true, then the Markov chain is said to be irreducible.\n",
    "We could see that from the above state diagram (displayed in the starting part of the notebook) ,\n",
    "We can reach to any other state in finite transitions. Hence its Irreducible, satsifying the conditon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Markov chain to be irreducible, we don’t have to be able to reach every state right away. It could be the case that it takes several steps to reach a certain state. But the important thing is that eventually we could reach any state with enough steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Condition 2: Aperiodic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we can’t get stuck cycling back and forth between the same set of states at regular intervals. In other words, Markov chain must be aperiodic.\n",
    " A Markov chain is periodic when we keep ending up at the same state every 2 or 3 or more regular intervals of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see that from the above state diagram (displayed in the starting part of the notebook) , We can get stuck cycling back and forth between the same set of states at regular intervals ,which is not satsifying the condition , inferring its  not aperiodic (i.e periodic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When these two conditions are satisfied — that is, when our Markov chain is both irreducible and aperiodic — then we can say our Markov chain is ergodic. And if we put all this together, we have the ergodic theorem, which says that any Markov chain that’s ergodic converges in the long run to a single stationary distribution regardless of our initial distribution.\n",
    "\n",
    "In other words, if we run one of these special types of Markov chains over many time periods, we’ll get closer and closer to a certain distribution regardless of how we started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPLICATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Google PageRank: The entire web can be thought of as a Markov model, where every web page can be a state and the links or references between these pages can be thought of as, transitions with probabilities\n",
    "\n",
    "2.Typing Word Prediction: Markov chains are known to be used for predicting upcoming words. They can also be used in auto-completion and suggestions.\n",
    "\n",
    "3.Text generator: Markov chains are most commonly used to generate dummy texts or produce large essays and compile speeches. It is also used in the name generators that you see on the web."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
